import nltkimport reimport osimport jsonimport aiohttpimport asyncioimport openpyxlfrom bs4 import BeautifulSoupfrom collections import Counterfrom nltk.corpus import stopwordsnltk.download('stopwords')STOPWORDS = set(stopwords.words('english'))# Function to generate SEO keywords from title and descriptiondef generate_seo_keywords(title, description, num_keywords=5):    combined_text = f"{title} {description}".lower()    words = re.findall(r'\b\w+\b', combined_text)    filtered_words = [word for word in words if word not in STOPWORDS and len(word) > 2]    word_freq = Counter(filtered_words)    common_words = [word for word, _ in word_freq.most_common(num_keywords)]    return ", ".join(common_words)# Function to generate meta description based on title and descriptiondef generate_meta_description(title, description, max_length=150):    combined_text = f"{title}. {description}"    return (combined_text[:max_length] + "...") if len(combined_text) > max_length else combined_text# Asynchronous function to scrape technical specifications from a product pageasync def scrape_technical_specifications(session, url):    try:        print(f"Scraping Product from URL: {url}")        async with session.get(url) as response:            html_content = await response.text()            soup = BeautifulSoup(html_content, 'html.parser')            # Find the parent div containing both tables            table_divider = soup.find('div', {'class': 'table__divider'})            if not table_divider:                return "<p>No technical specifications available.</p>"            # Find the two divs containing the tables            table_divs = table_divider.find_all('div', {'class': 'table__cont'})            if len(table_divs) < 2:                return "<p>No technical specifications available.</p>"            # Function to scrape a table from a div, excluding 'SKU' rows            def scrape_table(table_div):                table = table_div.find('table')                rows = table.find_all('tr') if table else []                table_data = []                for row in rows:                    cols = row.find_all('td')                    if len(cols) == 2:                        specification = cols[0].text.strip()                        value = cols[1].text.strip()                        # Exclude rows where specification contains 'SKU' (case-insensitive)                        if "sku" not in specification.lower():                            table_data.append((specification, value))                return table_data            # Scrape both tables            specifications_table1 = scrape_table(table_divs[0])            specifications_table2 = scrape_table(table_divs[1])            # Combine both tables' data            all_specifications = specifications_table1 + specifications_table2            # Create HTML table for Divi            table_html = "<table class='technical-specs'><thead><tr><th>Specification</th><th>Value</th></tr></thead><tbody>"            for spec, value in all_specifications:                table_html += f"<tr><td>{spec}</td><td>{value}</td></tr>"            table_html += "</tbody></table>"            return table_html    except Exception as e:        print(f"Error scraping product at URL {url}: {e}")        return "<p>Error fetching specifications.</p>"# Function to process JSON data and write to Excelasync def json_to_sheet(folder_path, excel_file):    try:        print("Storing products to Excel sheet...")        workbook = openpyxl.Workbook()        sheet = workbook.active        sheet.title = "Extracted Data"        headers = [            "CATEGORY", "TITLE", "DESCRIPTION", "TAGS", "PRICE",            "IMAGE SRC", "TECHNICAL SPECIFICATION", "SEO KEYWORDS", "META DESCRIPTION"        ]        sheet.append(headers)        # Start an aiohttp session        async with aiohttp.ClientSession() as session:            for file_name in os.listdir(folder_path):                if file_name.endswith(".json"):                    json_file_path = os.path.join(folder_path, file_name)                    try:                        print(f"Processing file: {file_name}")                        with open(json_file_path, "r") as file:                            json_data = json.load(file)                        category = os.path.splitext(file_name)[0]                        if isinstance(json_data, dict):                            products = [json_data]                        elif isinstance(json_data, list):                            products = json_data                        else:                            print(f"Skipping {file_name}: JSON is not in a valid format.")                            continue                        # Process each product in the file                        for product in products:                            title = product.get("title", "N/A")                            body_html = product.get("body_html", "N/A")                            if body_html != "N/A":                                body_html = re.sub(r'^<p>|</p>$', '', body_html)                            tags = ", ".join(product.get("tags", []))                            price = "N/A"                            if product.get("variants") and isinstance(product["variants"], list):                                price = product["variants"][0].get("price", "N/A")                            image_src = "N/A"                            if product.get("images") and isinstance(product["images"], list):                                image_src = product["images"][0].get("src", "N/A")                            handle = product.get("handle", "N/A")                            if handle != "N/A":                                product_url = f"https://buildpro.store/products/{handle}"                                technical_specification = await scrape_technical_specifications(session, product_url)                            else:                                technical_specification = "<p>No specifications found.</p>"                            # Generate SEO keywords and meta description                            seo_keywords = generate_seo_keywords(title, body_html)                            meta_description = generate_meta_description(title, body_html)                            sheet.append([category, title, body_html, tags, price,                                          image_src, technical_specification, seo_keywords, meta_description])                    except json.JSONDecodeError:                        print(f"Skipping {file_name}: Invalid JSON format.")                    except Exception as e:                        print(f"Error processing {file_name}: {e}")        # Save the workbook to the specified Excel file        workbook.save(excel_file)        print(f"Data successfully written to {excel_file}")    except PermissionError:        print(f"Permission denied: Ensure the file '{excel_file}' is closed and try again.")    except Exception as e:        print(f"An error occurred: {e}")# Main executionif __name__ == "__main__":    folder_path = r"E:\\PycharmProjects\\Web_Scrape\\URL_to_JSON\\product_data"    excel_file_path = "ProductsData.xlsx"    # Run the asynchronous function    asyncio.run(json_to_sheet(folder_path, excel_file_path))